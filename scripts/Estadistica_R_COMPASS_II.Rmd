---
title: "COMPASS: Estadística básica con R, parte II"
author: "Karla Vasco"
date: "02 de septiembre, 2021"
output:
  html_document:
    toc: TRUE
---
# * Taller de RStats: Parte II *
- Distribuciones de probabilidad, asimetría y curtosis, transformaciones básicas con gráficos
- Pruebas estadísticas paramétricas básicas (por ejemplo, t.test, ANOVA) y no paramétricas
- Estadística inferencial: prueba de hipótesis, valor p, correlaciones, intervalos de confianza
- Durante el taller, Sarah destacará las diferencias entre Excel / SPSS vs R


# Instalar paquetes si es necesario
```{r}
#install.packages("tidyverse")
#install.packages("psych")
```


# Cargando paquetes
```{r}
library(tidyverse) # paquete para trabajar con organización datos y visualizaciones
# O... carga los paquetes individualmente:
#library(readr)    # parte de tidyverse
#library(tidyr)    # parte de tidyverse
#library(dplyr)    # parte de tidyverse
#library(ggplot2)  # parte de tidyverse
#library(purrr)    # parte de tidyverse

library(readxl)    # para leer archivos de Excel
library(psych)     # tiene algunas funciones estadísticas útiles
library(here)      # Para encontrar tus archivos; establecer el directorio de trabajo
library(gapminder) # Base de datos Gapminder: esperanza de vida, PIB per cápita, población por país

```

## ¿Tienes tu propia base de datos?
Si deseas utilizar tu base de datos propia es mejor si esta guardada como texto sin formato. Debe tener una fila con todos los nombres de tus columnas/variables, luego todos sus datos. 

## ¿Migrando desde Excel?
Si estás usando Excel es más fácil guardar tu archivo en formato CSV (Archivo> Guardar como - o presione F12 - luego en "Guardar como tipo" en el menú desplegable, cámbialo a "CSV delimitado por comas (CSV) ")

# La ruta sería relativa a su punto de partida / ubicación de su archivo R
```{r}
here::here() ## Apunta al lugar donde abrió su sesión de RStudio.
```

#Importa tu base de datos 
```{r}
mis_datos <- read_csv("ruta/a/tus/datos", # selecciona un archivo de tu computador
                      header=TRUE,      # los nombres de las columnas estan en la primera fila
                      na=c("N/A","NA","None","")) # Dice a R como identificar valores faltantes y los reemplaza con NA (en color gris)
```

## Gapminder
cargar y examinar la base de datos de Gapminder
Sin embargo, esto no es típico. La mayoría de los paquetes no tienen conjuntos de datos de ejemplo.
Tidyverse y Gapminder tienen conjuntos de datos de muestra.
```{r}
View(gapminder)
str(gapminder) # examina la estructura de la base de datos
glimpse(gapminder) # ...
summary(gapminder) # estadistica basica por columnas
```

# DISTRIBUCIONES
###################
# psych::describe() # works without loading
describe(gapminder$lifeExp) # see stats at a glance, requires 'psych' R package
description <- describe(gapminder) # saves describe as object called 'description'
description$min # to look at a specific measure across variables

# hist(gapminder$lifeExp) # histogram
hist(gapminder$lifeExp)
ggplot(gapminder, aes(x=lifeExp)) +
  geom_histogram() +
  theme_bw()

description$skew        # Skewness: left/right trend of distribution curve
description$kurtosis    # Kurtosis: peakedness/flatness of distribution curve

## Other ways to compute skewness and kurtosis
# skew(gapminder$lifeExp) # skewness value for Life Expectancy
# kurtosi(gapminder$lifeExp) # kurtosis value for Life Expectancy

#####################
## TRANSFORMATIONS ##
#####################

# You may want to rescale data if the tests you want to use assume normally-distributed data or you have different measures along very different scales. See below for "Testing for Normality"

## Log transformation
gapminder$log.pop <- log(gapminder$pop, base=10)
logpop_hist <- ggplot(gapminder, aes(x=log.pop)) +
  geom_histogram() +
  # geom_density() +
  theme_bw()
logpop_hist
# Alternatively...
ggplot(gapminder, aes(x=pop)) +
  geom_histogram() +
  scale_x_log10()

# log is BASE E by default, NOT BASE 10

showinglogtransformation <- log10(gapminder$pop)
describe(showinglogtransformation)
hist(showinglogtransformation)

## Rescaling variables
View(gapminder)

GDP_rescaled <- scale(gapminder$gdpPercap) # 'scale' available in base R; transform your data to z-scores
View(GDP_rescaled)

## Rescaling w/ Psych package
#library(psych) #load if needed
poprescaled <- rescale(gapminder$pop,m=50,sd=15) # rescale specifying a certain mean and standard deviation
View(poprescaled)
describe(poprescaled)


#### Testing for normality ####
## Is your data normally distributed?
## Checking normality, goodness of fit, independence/difference tests

gapminder %>%
  ggplot(aes(x = lifeExp)) +
  geom_density() +
  stat_function(fun = dnorm,
                args = list(mean = mean(gapminder$lifeExp),
                            sd = sd(gapminder$lifeExp)),
                color = "red")

## viewing the distribution
pop_hist <- ggplot(gapminder, aes(x=pop)) +
  geom_histogram() +
  # geom_density() +
  theme_bw() # no grey background
pop_hist

## Shapiro-Wilk test
shapiro.test(gapminder$lifeExp) # gapminder Life Expectancy is nonnormally distributed.  p > .05 would indicate null hyp cannot be rejected and data are normally distributed.
# significant --> non-normal

## Kolmogorov-Smirnov test
ks.test(gapminder$lifeExp,
        "pnorm",                       # what you are testing for
        mean=mean(gapminder$lifeExp),  #
        sd=sd(gapminder$lifeExp))      # Kolmogorov-Smirnov test needs three additional arguments: the distribution to test against (pnorm), the M, and the SD
# significant --> non-normal
# you may get an error from R for the ks.test that says "ties should not be present"; it does not like multiple hits of the same number
# KS vs Shapiro: https://stats.stackexchange.com/questions/362/what-is-the-difference-between-the-shapiro-wilk-test-of-normality-and-the-kolmog

# for data that are ordinal, are nonnormal, or have outliers
median(gapminder$lifeExp)

# most useful only for normally-distributed data
mean(gapminder$lifeExp)

pop_hist +
  geom_vline(xintercept=mean(gapminder$pop), color="red") +
  geom_vline(xintercept=median(gapminder$pop), color="blue") +
  scale_x_log10()

sd(gapminder$lifeExp)
var(gapminder$lifeExp) # variance (=sd^2)

#SD vs SE
# https://www.r-bloggers.com/standard-deviation-vs-standard-error/
# here, instead of gapminder, we're using sample sets of numbers we're calling R objects "a" and "b"
a <- c(-0.73,-0.3,0.01,0.25,0.59) # sample test scores (0 being average probability of examinee success)
b <- c(-0.3,-0.13,-0.02,0.19,0.41) # sample standard errors
mypretenddata <- as.data.frame(cbind(a, b)) # bind a and b together, each in their own column - as a data frame
View(mypretenddata)

#Plotting line/bar plot with the standard error added
mypretenddata_2 <- barplot(mypretenddata$a,ylim=c(-1.5,1.5))
arrows(x0=mypretenddata_2,y0=mypretenddata$a+mypretenddata$b,y1=mypretenddata$a-mypretenddata$b,angle=90,code=3,length=0.25)


# plot to show confidence intervals around the mean (may not be so useful)
# err <- error.dots(gapminder$lifeExp)
# err$des   # ...
# err$order # rank ordered?


################################
## Correlation/Association tests
################################

## for post-hoc tests, see the leveneTest command in the 'car' package.
# leveneTest(DV ~ IV, data = name_of_your_dataset_here)
## Or see Dani Navarro's "R for Psychological Science" 20.4.1
# http://psyr.org/introductory-statistics.html


### for effect sizes, check out 'compute.es' package
install.packages('compute.es') # one-time
library(compute.es)
?`compute.es-package`

# correlations - Pearson is default
cor(gapminder$lifeExp,gapminder$pop)

cor(gapminder$lifeExp,gapminder$pop,
    method="spearman") # to change the method

# if you have incomplete data, you'll need to add the "use" argument to cor to specify to R whether to correlate pairwise, use only complete observations, remove NAs, or another stipulation.


## more detailed correlations (needs 'psych' package)
corr.test(gapminder$lifeExp,gapminder$pop)


## Correlation & confidence intervals
print(corr.test(gapminder$lifeExp,gapminder$pop),
      short=FALSE) #corr.test requires 'psych' package




## We may touch on chi-sq, t-tests, ANOVAs if time permits





# so we have simplified data to compare for statistical tests:

# sample 'n' rows from gapminder data
#library(dplyr) #load package if needed
abbrevdata_1 <- sample_n(gapminder,size=20)
abbrevdata_2 <- sample_n(gapminder,size=20)

# or sample specific records (requires 'dplyr')
View(gapminder)
abbrevdata_1 <- gapminder %>%
  filter(year == "1982")  #rows with data from the year 1982
abbrevdata_2 <- gapminder %>%
  filter(year == "1987")  #rows with data from the year 1987

data_shortened <- table(abbrevdata_1$lifeExp,abbrevdata_2$lifeExp)


## chi-square statistical test
test <- chisq.test(data_shortened, correct=FALSE); test
test$residuals
sqrt(test$statistic/sum(data_shortened)*(min(dim(data_shortened))-1)) # effect size: Cramer's V
test$expected



## t-test
t.test(data_shortened, mu=0) # if normality had not been violated, we would use this line. Assumes UNequal variances by default (can add var.equal=TRUE to specify equal variances)


# instead, use Wilcoxon:
wilcox.test(data_shortened, mu=0, correct=FALSE)


#### ANOVA ???
model_aov <- aov(lifeExp ~ continent, data=gapminder)
broom::tidy(model_aov)
#aov(DV ~ IV, data=dataset)
#model.01<-aov(DV ~ IV, data=dataset); summary(model.01)
model_tukey <- TukeyHSD(model_aov)
broom::tidy(model_tukey)

# Kruskal-Wallis
#kruskal.test(DV ~ IV, data=dataset)
?stats::kruskal.test



##linear regression with 1 predictor, sum contrasts (best for the sake of comparability with other software)
options(contrasts=c("contr.sum", "contr.poly")) # set sum contrasts
#model.01 <- lm(DV ~ IV, data=dataset)

#summary(model.01) # determine the nature of the effect(s) numerically

#drop1(model.01, test="F") # for p values ; with multiple variables, tells you which predictor can be dropped

#NOTE CASE of Anova different from anova
#Anova(model.01, type="III") # compute ANOVA of model with type III sums of squares. Same as SPSS. In 'car' library





##  power analyses
#install.packages('pwr') #if needed
library(pwr)

#for instructions CORRELATIONS
help(pwr.r.test)

#for correlation power posteriori
pwr.r.test(n=--, # n = number of participants
           r=.58,
           sig.level=.05,
           alternative=c("two.sided"))
#for correlation power a priori. How big of an N do you need? [we want a medium effect size (.30)]
pwr.r.test(r = .30, sig.level = 0.05, power = .80)
#for correlation power a priori. How big of an N do you need for one-tailed?
pwr.r.test(r = .30, sig.level = 0.05, power = .80, alternative = c("greater"))

#for instructions ANOVA
help(pwr.anova.test)

#for posteriori
pwr.anova.test(k = 4, n = 10, f = .432, sig.level = 0.05)
#for a priori
pwr.anova.test(k = 4, f = .4, sig.level = 0.05, power = .80)

#for instructions t-test
help(pwr.t.test)

#for posteriori
pwr.t.test(n=10, d=1.93, type=c("two.sample"))